{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7886c320-b714-48a0-9ebc-2621e6ee9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f902bd9b-7d31-47a6-b420-3f11ad0a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Varshil\\Downloads\\comments_data (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01690260-e2c1-43a7-94b4-6121f635ab35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>reply_comment_id</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugz9iIF4cwCpLJd-Jl54AaABAg</td>\n",
       "      <td>2024-07-15T16:13:38Z</td>\n",
       "      <td>@adityasaha4704</td>\n",
       "      <td>I thank you for my very existence</td>\n",
       "      <td>2024-07-15T16:13:38Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgxnDVp1OB0v5Wf8iD54AaABAg</td>\n",
       "      <td>2023-09-18T19:14:31Z</td>\n",
       "      <td>@tippydippy6529</td>\n",
       "      <td>every couple of weeks I find myself watching t...</td>\n",
       "      <td>2023-09-18T19:14:30Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugyw_I56xAhWu9LctcV4AaABAg</td>\n",
       "      <td>2023-07-17T16:54:37Z</td>\n",
       "      <td>@RamonSBK</td>\n",
       "      <td>Vagabond Reminds me everyday that anything is ...</td>\n",
       "      <td>2023-09-07T12:22:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgxWtjYys9HNS7ZxRap4AaABAg</td>\n",
       "      <td>2023-07-31T16:22:14Z</td>\n",
       "      <td>@wubbalubbadubdub777</td>\n",
       "      <td>ayahviv ! oued amizour days a rayan !</td>\n",
       "      <td>2023-07-31T16:22:14Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugwu8Jy2t39SzRnUjoV4AaABAg</td>\n",
       "      <td>2023-07-19T23:03:47Z</td>\n",
       "      <td>@angelagrullon2189</td>\n",
       "      <td>La vibes que transmite esta cancion mierda.......</td>\n",
       "      <td>2023-07-19T23:03:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CommentID             Timestamp              Username  \\\n",
       "0  Ugz9iIF4cwCpLJd-Jl54AaABAg  2024-07-15T16:13:38Z       @adityasaha4704   \n",
       "1  UgxnDVp1OB0v5Wf8iD54AaABAg  2023-09-18T19:14:31Z       @tippydippy6529   \n",
       "2  Ugyw_I56xAhWu9LctcV4AaABAg  2023-07-17T16:54:37Z             @RamonSBK   \n",
       "3  UgxWtjYys9HNS7ZxRap4AaABAg  2023-07-31T16:22:14Z  @wubbalubbadubdub777   \n",
       "4  Ugwu8Jy2t39SzRnUjoV4AaABAg  2023-07-19T23:03:47Z    @angelagrullon2189   \n",
       "\n",
       "                                             Comment                  Date  \\\n",
       "0                  I thank you for my very existence  2024-07-15T16:13:38Z   \n",
       "1  every couple of weeks I find myself watching t...  2023-09-18T19:14:30Z   \n",
       "2  Vagabond Reminds me everyday that anything is ...  2023-09-07T12:22:47Z   \n",
       "3              ayahviv ! oued amizour days a rayan !  2023-07-31T16:22:14Z   \n",
       "4  La vibes que transmite esta cancion mierda.......  2023-07-19T23:03:47Z   \n",
       "\n",
       "   Likes reply_comment_id replies  \n",
       "0      0              NaN     NaN  \n",
       "1      2              NaN     NaN  \n",
       "2      0              NaN     NaN  \n",
       "3      0              NaN     NaN  \n",
       "4      0              NaN     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f49a52-ff2a-4235-91ee-1e573eb510ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex # re2 for faster matching and replacing or removing\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "def sent_process(sent):\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    for i, word in enumerate(words):\n",
    "        # Remove various types of URLs\n",
    "        word = word.lower()\n",
    "        word = regex.sub(r'\\p{Emoji}', lambda m: emoji.demojize(m.group()), word)\n",
    "        word = regex.sub(r'@[A-Za-z0-9]+\\b', '', word)  # rplace usernames from the comments\n",
    "        word = regex.sub(r'https?://\\S+', '', word)  # Remove HTTP/HTTPS links\n",
    "        word = regex.sub(r'[^a-zA-Z0-9\\s]', ' ', word)\n",
    "        words[i] = word.strip()\n",
    "        \n",
    "    return \" \".join(words)\n",
    "    \n",
    "def text_process(df):\n",
    "    df['cleaned_parent_comm'] = [sent_process(x) for x in df.Comment]\n",
    "    #df['cleaned_replies'] = [\" || \".join([sent_process(y) for y in x.split(\" || \") if x else np.nan]) for x in df.replies]\n",
    "\n",
    "    df['cleaned_replies'] = df['replies'].apply(\n",
    "        lambda x: \" || \".join([sent_process(y) for y in x.split(\" || \")]) if pd.notna(x) else np.nan\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "df = text_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6f73a7-1242-4c14-9e20-6e23d3ce8081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>reply_comment_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>cleaned_parent_comm</th>\n",
       "      <th>cleaned_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugz9iIF4cwCpLJd-Jl54AaABAg</td>\n",
       "      <td>2024-07-15T16:13:38Z</td>\n",
       "      <td>@adityasaha4704</td>\n",
       "      <td>I thank you for my very existence</td>\n",
       "      <td>2024-07-15T16:13:38Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i thank you for my very existence</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgxnDVp1OB0v5Wf8iD54AaABAg</td>\n",
       "      <td>2023-09-18T19:14:31Z</td>\n",
       "      <td>@tippydippy6529</td>\n",
       "      <td>every couple of weeks I find myself watching t...</td>\n",
       "      <td>2023-09-18T19:14:30Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>every couple of weeks i find myself watching t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugyw_I56xAhWu9LctcV4AaABAg</td>\n",
       "      <td>2023-07-17T16:54:37Z</td>\n",
       "      <td>@RamonSBK</td>\n",
       "      <td>Vagabond Reminds me everyday that anything is ...</td>\n",
       "      <td>2023-09-07T12:22:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vagabond reminds me everyday that anything is ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgxWtjYys9HNS7ZxRap4AaABAg</td>\n",
       "      <td>2023-07-31T16:22:14Z</td>\n",
       "      <td>@wubbalubbadubdub777</td>\n",
       "      <td>ayahviv ! oued amizour days a rayan !</td>\n",
       "      <td>2023-07-31T16:22:14Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ayahviv  oued amizour days a rayan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugwu8Jy2t39SzRnUjoV4AaABAg</td>\n",
       "      <td>2023-07-19T23:03:47Z</td>\n",
       "      <td>@angelagrullon2189</td>\n",
       "      <td>La vibes que transmite esta cancion mierda.......</td>\n",
       "      <td>2023-07-19T23:03:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>la vibes que transmite esta cancion mierda  y ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CommentID             Timestamp              Username  \\\n",
       "0  Ugz9iIF4cwCpLJd-Jl54AaABAg  2024-07-15T16:13:38Z       @adityasaha4704   \n",
       "1  UgxnDVp1OB0v5Wf8iD54AaABAg  2023-09-18T19:14:31Z       @tippydippy6529   \n",
       "2  Ugyw_I56xAhWu9LctcV4AaABAg  2023-07-17T16:54:37Z             @RamonSBK   \n",
       "3  UgxWtjYys9HNS7ZxRap4AaABAg  2023-07-31T16:22:14Z  @wubbalubbadubdub777   \n",
       "4  Ugwu8Jy2t39SzRnUjoV4AaABAg  2023-07-19T23:03:47Z    @angelagrullon2189   \n",
       "\n",
       "                                             Comment                  Date  \\\n",
       "0                  I thank you for my very existence  2024-07-15T16:13:38Z   \n",
       "1  every couple of weeks I find myself watching t...  2023-09-18T19:14:30Z   \n",
       "2  Vagabond Reminds me everyday that anything is ...  2023-09-07T12:22:47Z   \n",
       "3              ayahviv ! oued amizour days a rayan !  2023-07-31T16:22:14Z   \n",
       "4  La vibes que transmite esta cancion mierda.......  2023-07-19T23:03:47Z   \n",
       "\n",
       "   Likes reply_comment_id replies  \\\n",
       "0      0              NaN     NaN   \n",
       "1      2              NaN     NaN   \n",
       "2      0              NaN     NaN   \n",
       "3      0              NaN     NaN   \n",
       "4      0              NaN     NaN   \n",
       "\n",
       "                                 cleaned_parent_comm cleaned_replies  \n",
       "0                  i thank you for my very existence             NaN  \n",
       "1  every couple of weeks i find myself watching t...             NaN  \n",
       "2  vagabond reminds me everyday that anything is ...             NaN  \n",
       "3                ayahviv  oued amizour days a rayan              NaN  \n",
       "4  la vibes que transmite esta cancion mierda  y ...             NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e346ec0-051c-4aac-8f5c-ed9db49dcfb0",
   "metadata": {},
   "source": [
    "### 3 types of data we need : \n",
    "#### 1. all comments ==> topic modelling for top discussed topics\n",
    "#### 2. all comments ==> sentiment analysis\n",
    "#### 3. top 5 comments and replies ==> summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e100c11d-00c6-46d2-a5e6-f08be0c30cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = list(df.cleaned_parent_comm) +  list(df.cleaned_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f357527f-5370-4e46-867f-8bfb4534535a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CommentID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>reply_comment_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>cleaned_parent_comm</th>\n",
       "      <th>cleaned_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>Ugzac0O_LaZWK1ZtsCd4AaABAg</td>\n",
       "      <td>2021-07-10T11:23:55Z</td>\n",
       "      <td>@MR.X-l2u</td>\n",
       "      <td>Vegabond and Berserk are the two best manga ev...</td>\n",
       "      <td>2021-07-10T11:23:55Z</td>\n",
       "      <td>2892</td>\n",
       "      <td>|| UgxnDVp1OB0v5Wf8iD54AaABAg || UgxWtjYys9HN...</td>\n",
       "      <td>|| every couple of weeks I find myself watchi...</td>\n",
       "      <td>vegabond and berserk are the two best manga ev...</td>\n",
       "      <td>|| every couple of weeks i find myself watchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>UgzyQhJPJ-zHhKkDN0B4AaABAg</td>\n",
       "      <td>2021-08-30T17:34:52Z</td>\n",
       "      <td>@cheeseman2219</td>\n",
       "      <td>Musashi vs 70 Yoshioka men was the most memora...</td>\n",
       "      <td>2021-08-30T17:34:52Z</td>\n",
       "      <td>1176</td>\n",
       "      <td>|| UgztSsIAWzxaZHhlgYx4AaABAg || UgwudFWY-fxY...</td>\n",
       "      <td>|| You have to walk through the depths of Hel...</td>\n",
       "      <td>musashi vs 70 yoshioka men was the most memora...</td>\n",
       "      <td>|| you have to walk through the depths of hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192</td>\n",
       "      <td>Ugx_LdgI0vWOumWYS_d4AaABAg</td>\n",
       "      <td>2021-07-10T05:49:27Z</td>\n",
       "      <td>@abandonaccount8582</td>\n",
       "      <td>Vegabond is underrated more people should read...</td>\n",
       "      <td>2021-07-10T05:49:27Z</td>\n",
       "      <td>1158</td>\n",
       "      <td>|| UgwbxjfdeS8E9KEuCsl4AaABAg || Ugwl4x2FegQ3...</td>\n",
       "      <td>|| This man is my inspiration ðŸ’¯ðŸ”¥ || what a gr...</td>\n",
       "      <td>vegabond is underrated more people should read...</td>\n",
       "      <td>|| this man is my inspiration hundred points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161</td>\n",
       "      <td>UgypwPJSMIyeogONpxl4AaABAg</td>\n",
       "      <td>2021-10-27T10:47:26Z</td>\n",
       "      <td>@DavionX13</td>\n",
       "      <td>Preoccupied with a single leaf, you won't see ...</td>\n",
       "      <td>2021-10-27T10:47:26Z</td>\n",
       "      <td>928</td>\n",
       "      <td>|| Ugz9iIF4cwCpLJd-Jl54AaABAg || Ugx4ws8nBg-k...</td>\n",
       "      <td>|| I thank you for my very existence || he is...</td>\n",
       "      <td>preoccupied with a single leaf  you wo n t see...</td>\n",
       "      <td>|| i thank you for my very existence || he is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>186</td>\n",
       "      <td>UgxMNe48oT4QElg2klN4AaABAg</td>\n",
       "      <td>2021-07-21T01:06:14Z</td>\n",
       "      <td>@sovereignrepublicofcopticx6397</td>\n",
       "      <td>The best part is that he was real.</td>\n",
       "      <td>2021-07-21T01:06:14Z</td>\n",
       "      <td>913</td>\n",
       "      <td>|| Ugxm-p8LiLGYRbc0wFd4AaABAg || UgxYCUvdzw2a...</td>\n",
       "      <td>|| The fact that beardless/young Musashi's fa...</td>\n",
       "      <td>the best part is that he was real</td>\n",
       "      <td>|| the fact that beardless young musashi s fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                   CommentID             Timestamp  \\\n",
       "0    190  Ugzac0O_LaZWK1ZtsCd4AaABAg  2021-07-10T11:23:55Z   \n",
       "1    180  UgzyQhJPJ-zHhKkDN0B4AaABAg  2021-08-30T17:34:52Z   \n",
       "2    192  Ugx_LdgI0vWOumWYS_d4AaABAg  2021-07-10T05:49:27Z   \n",
       "3    161  UgypwPJSMIyeogONpxl4AaABAg  2021-10-27T10:47:26Z   \n",
       "4    186  UgxMNe48oT4QElg2klN4AaABAg  2021-07-21T01:06:14Z   \n",
       "\n",
       "                          Username  \\\n",
       "0                        @MR.X-l2u   \n",
       "1                   @cheeseman2219   \n",
       "2              @abandonaccount8582   \n",
       "3                       @DavionX13   \n",
       "4  @sovereignrepublicofcopticx6397   \n",
       "\n",
       "                                             Comment                  Date  \\\n",
       "0  Vegabond and Berserk are the two best manga ev...  2021-07-10T11:23:55Z   \n",
       "1  Musashi vs 70 Yoshioka men was the most memora...  2021-08-30T17:34:52Z   \n",
       "2  Vegabond is underrated more people should read...  2021-07-10T05:49:27Z   \n",
       "3  Preoccupied with a single leaf, you won't see ...  2021-10-27T10:47:26Z   \n",
       "4                 The best part is that he was real.  2021-07-21T01:06:14Z   \n",
       "\n",
       "   Likes                                   reply_comment_id  \\\n",
       "0   2892   || UgxnDVp1OB0v5Wf8iD54AaABAg || UgxWtjYys9HN...   \n",
       "1   1176   || UgztSsIAWzxaZHhlgYx4AaABAg || UgwudFWY-fxY...   \n",
       "2   1158   || UgwbxjfdeS8E9KEuCsl4AaABAg || Ugwl4x2FegQ3...   \n",
       "3    928   || Ugz9iIF4cwCpLJd-Jl54AaABAg || Ugx4ws8nBg-k...   \n",
       "4    913   || Ugxm-p8LiLGYRbc0wFd4AaABAg || UgxYCUvdzw2a...   \n",
       "\n",
       "                                             replies  \\\n",
       "0   || every couple of weeks I find myself watchi...   \n",
       "1   || You have to walk through the depths of Hel...   \n",
       "2   || This man is my inspiration ðŸ’¯ðŸ”¥ || what a gr...   \n",
       "3   || I thank you for my very existence || he is...   \n",
       "4   || The fact that beardless/young Musashi's fa...   \n",
       "\n",
       "                                 cleaned_parent_comm  \\\n",
       "0  vegabond and berserk are the two best manga ev...   \n",
       "1  musashi vs 70 yoshioka men was the most memora...   \n",
       "2  vegabond is underrated more people should read...   \n",
       "3  preoccupied with a single leaf  you wo n t see...   \n",
       "4                 the best part is that he was real    \n",
       "\n",
       "                                     cleaned_replies  \n",
       "0   || every couple of weeks i find myself watchi...  \n",
       "1   || you have to walk through the depths of hel...  \n",
       "2   || this man is my inspiration hundred points ...  \n",
       "3   || i thank you for my very existence || he is...  \n",
       "4   || the fact that beardless young musashi s fa...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments_df = df.sort_values(by = 'Likes', ascending = False).reset_index()\n",
    "top_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa0ce02-d9e1-4b36-b0e0-f1193af3f266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': 'vegabond and berserk are the two best manga ever made ',\n",
       " 'replies': ' || every couple of weeks i find myself watching this on repeat  have for a couple of years now    great job || ayahviv  oued amizour days a rayan  || la vibes que transmite esta cancion mierda  y mas con este manga que es una verdadera grasa || vagabond reminds me everyday that anything is possible and when people say it is n t it means it s not possible for them because they feel like it isnt || when musashi said  i am vagabond   i felt that  || big sad || how to download english version pdg for free || thankyou i will use these clips for my edit and i will give credit on tiktok || les meilleurs planches se berserk et vagabond || a good read smiling face with smiling eyes || just waiting to watch the insanely animated version of  vagabond  || eu tbm  depois que eu conhe i esse manga minha vida mudo por completo || witch remix is this  ||   uzochiokeke4328 lmaoooo ||   josephisrael507 i was just trying to say that these manga s art is so beautiful that people cant make an anime adaptation and keep it at the art level of the manga just praising miura and inoue || every page s of vagabond is a masterpiece  pls bring back vagabond inoue sensei || oloko ficou muito bom || thanks for this video  i saw this video in my feed randomly about when it was published and i decided to then read vagabond  vagabond totally changed my view on life  just wanted to say thanks for this masterpiece  || anime name || il sont tellement beau les dessin || cool swordsman and cool music || this manga is too good to even get an anime || i wish you the best of luck beaming face with smiling eyes || you have a goal  i will follow your work with hopes then  good luck yakohiro ||   angelodewitt6336 thank you so much  i m gon na never forget about you  and all the people that have believed in my dreams || hope you for the best boss  || become invincible under the sun  || i ll waiting for you brother || good luck  if you ever do it  come back here and post the name of your future manga  i will read it || good luck  but do n t try to be like someone  do something of your own  || tell us the amazing journey after you become successful  all the best OK hand  light skin tone || i m looking forward to it brother  || muito bom oncoming fist  light skin tone || baki || adding this and berserk to the must read list || i swear im straight but after this i m not so sure anymore || never heard of this manga till now  imma check it out   ||   zezqi true man some manga to anime adaptations just don  t work and they destroy any chance of having new fans || one of the best manga what i read and of course berserk too  || ah it has sadly not the vibe like the ww2 german b roll videos || yo  where the part where he fights a neanderthal  || i miss 14k views  || cagaron al manga con esa canci n del culo || vagabond is a poetic healing book || musashi s so great he s been refrenced in many other anime too face with thermometer  face with thermometer || where can i read this  || i know mushashi from baki skull and crossbones || there is a final chapter that inoue himself drew at some event called the last   it skips right to the end and does n t really explain what happened in between the end of the farming arc and the finale but its some type of closure if you will  if you have n t yet  i definitely recommend checking it out  || semi biography of musashi miyamoto the greatest swordsman of all time || i read through this series when i was a teenager  the artwork in the manga is an act of love in and in of itself  || that arc with the village  idk why but its the best for me || farming arc goat || could u tell me what version of little dark age is this  i cant find it || https  youtu be 7vgef6xccvm here dude || bro the art style that it has its so dang good || yasuo go back your line || new manga to read i see || 10 10 but big cliffhanger at the end || another manga to read smh  hopefully i do n t like it as much as chainsaw man || name  || for me it s one of the best  i like berserk more than vagabond  ||   shivam zg7lb in my opinion berserk takes a bit to become great  i  d say around 8 9 volumes  vagabond got me hooked from the beginning  but yeah these two  with kingdom  are the best manga || conserteza um dos melhores mang s que j li na minha vida red heart || one of the best manga i ve ever read || musashi was the original sigma male  || vegabond and berserk is the best manga for me you ca n t change my mind   ||  sausage with sauce well there s a last chapter inoue held an exhibition of the last one || this and gol d rogers quote are my favorite in manga || you should read the manga lone wolf and cub  || vagabond  vinland saga  and berserk are stories of progress and maturity  meanwhile in dorohedoro   sparkles  drugs  sparkles || dorohedoro is a good manga tho || love them all || dorohedoro is a depressing world with a funny protagonist  || this is like the realistic version of jojo || he fight and lived thtsall || people are crying right now about the fact that thorfinn does n t kill anymore  like wtf the story is about finding vinland saga  and thorfinn s character development is so good OK hand || are u saying that those are the 4 best seinen  ||   abdhdudjrujj3972 yes they carry tf out of seinen ||   remains10 loll  not even close ||   remains10 theyre not even close to carrying seinen  there are plenty of better seinen  maybe popularity wise but thats really it  ||   abdhdudjrujj3972 that s legit what carryng means  ||   remains10 i asked if the person was saying that those are the 4 best seinen and you said yes  ur contradicting yourself bro what ||   abdhdudjrujj3972 i think you might be misunderstanding my comment  i was n t being sarcastic im genuinely asking you because i wan na read more good stuff  ||   joaooliveira9993 well im just trolling  none of my comments are serious  but i do know a few incredible mangas that i really enjoyed  ||   joaooliveira9993  these are all seinen  seizon life  mystery  detective bambino  cooking  slice of life  drama apollos song  psychological  drama the man without talent  slice of life  drama boku to issho  comedy  slice of life glaucos  sports  this is absolutely peak  himizu  slice of life  psychological  horror  tragedy all of these are incredible  except boku to isssho  its just very funny  ||   abdhdudjrujj3972 best can mean multiple things  something being the most popular could be considered the best  especially considering when it comes to stuff like manga numbers of sales etc speaks louder than subjective personal opinions  || punpun is edgelord bullshit replace it with alita or monster ||   abdhdudjrujj3972 names ||   abdhdudjrujj3972 good taste especially bambino and munoe no hitto try shigurui  historie ||   sunrisevideosgamingandmuch7791 nah punpun goated i m sorry ||   jegangunnithan4565 historie is so good  i hope it comes off hiatus ||   rob526 idrc abt this argument anymore u win || why u left out monster  bro  ||   thelizardwizard the2nd double check again smiling face with horns || hey  atleast sasaki is a boy || i really want to find that thumbnail picture  some1 help || ty   heavenly father || amazing editing  i watch this videos dozen of time  ||   whatthehellisthis5634 musashi probably has even more merit swordmanship wise considering how guts had his inhuman strength meanwhile musashi is pure technique || he kept that spiral of death going ||   fogz8537 no  shinmen takez and miyamoto musashi are exactly the same person  || musashi is known for killing his opponents with only one slash  by using a wooden training sword ||   joatanpereira4272 no no friends  is nicolle the rock ||   kobz2862  thonk   ||  chris brown alright wait i havent read it yet no spoilers please im gon na start it today ||  chris brown ah ok then || sasaki  miyamoto ||   guydude439 rip swallow slayer || well  guts from berserk resembels a king in my country that lived in the middle ages  it is said that car du an  or dushan for translation  was 2 meters tall  muscular and had a long sword that he used with ease just like guts so its possible that kentaro used him as an example ||   billymaguire5769 i thought for a second that this was my comment  but then i saw the name  ||   guydude439 damn i just got spoiled confused face i really need ti stop looking in comments ||   kobz2862 ya he kills kojiro with a wooden sword in real life  || in allah can m ||   frog6054 amin || cant argue with facts || preach || a man of culture  ||   naufalgitaro4908 fair enough  but i do n t really follow when berserk has all those dark fantasy elements  i adore vinland saga  i think it s a beautiful series with a great message and core themes  also askeladd is my favorite character in anything ever tbh || when vinland saga ends it will be up there || bro i agree only with berserk  one piece and jojo ||   myk1172 guts || i personally like both and their in my top 5 but if i  m being fr for me it  s kingdom berserk solo leveling jagaaaaan vagabond ||   frog6054 eyyyy i was just saying ||   passionhillslive5115 i love oyasumi punpun so much  some of the best written characters ever in my opinion || cap  monster  pun pun  20th century boys  vinland saga  kingdom are all potential best mangas ||   mimocriminel8592 ive come back 2 months later to tell u that i was wrong  not that vagabonds the best manga of all time  but that my recommendations were not good and dont do the title  best of all time  justice  || and tokyo ghoul || add one piece and blade of the immortal to that || vagabond berserk kingdom and vinland saga  pure masterpiece ||    abdhdudjrujj3972 nah u just try to be different  doent mean your opinion diff than other  your opnion bcm valid why  just bcs all ppl agree wit the fact that vagabond and berserk is the best manga makes that opinion unvalid  its ok to be diff but still u got a gud taste no hate man just say what i want to say  || everyone here needs to read more manga  but yes i agree  vagabond is one of the best manga out there ||   armandopalermo5343 incredibly valid opinion ||   mimocriminel8592 kingdom || add kingdom  monster  21st century boys  pluto  || easily the best two that wo n t be complete  || vagabond  berserk  one piece  these are my favourite manga  || spittin || on my grandma bro berserk and vagabonds are so similair yet so different both are masterpieces at what they do || what about gantz  ||   bladeofimmortal7224 unfortunately no but i wish  ||   mr x l2u so the studios will adapt shitty rom coms but not a gem like vagabond  why  ||   mr x l2u i think madhouse  bones or mappa could adapt vagabond  what do you think  || even tho i am die hard tokyo ghoul fan  i agree || i think there probably some of the best ever but im sure theres better ngl  alot of manga are top tier and just havent been promoted  || the most valid statement of all time || correct  and poetically both doomed to perpetual hiatus ||   abdhdudjrujj3972 bruh stop  berserk is better than any of them  only kingdom  real ashita no joe and monster is better than vagabond ||   noopninja3733 cap  u not even know kingdom  monster  gintama  ashita no joe ||   abdhdudjrujj3972 and jojo ||   naufalgitaro4908 bro what  have u even read vinland  how tf thats not realistic  and berserk is more not realistic than vinland lmao || vegapunk call me hand  cat with tears of joy ||   laiheaux807 now that you mention him i ca n t wait to see what doctor vegapunk will do in one piece  ||   sepulchure796 nice || monster     ||   dekekeke true ||   anisurrahaman9969 nice joke aot is not even top 1000  it s trash  whereas baki is not even close to what vagabond and berserk is and jojo only part which is better is part 7 so stfu ||   abdhdudjrujj3972 bruh  no  || vindland saga || both gave me anxiety || grass is green ||   mr x l2u better to be not one  ||   dekekeke go touch grass  emo  ||   arthurdayne69 ok  boy ||   dekekeke okie dookey  poota  ||   bladeofimmortal7224 there s no vagabond adaptation for a simple reason  one  the art s way too good to catch up and potrait in animation  look at berserk  and how they butchered berserk s manga art in animation  second  miyamoto musashi  ||   abdhdudjrujj3972 respect ur opinion but ashita no joe negs imo || vagabond is amazing ||   abdhdudjrujj3972 read all and all those were mid good but dogshit compared to berserk and vagabond ||   xarhzx bruh all of the manga below were way better than goodnight punpun ||   vagabondvinlandsagashamoan2046 i was a sweaty elitist when i wrote that comment  ive seen the light but lwac is infinitely better than both still ||   mr x l2u i think jjba  manga  its better than berserk   ||   yunleung2631 well  they re two of the best manga of all time  and yes  you re right  there are many manga that can offer an experience as good as these two  ||   threesixnine8482 true || facts ||   michaelpotatoman1240 what is it about  ||   abdhdudjrujj3972 vagabond is better than everything here || vinland saga too ||   birer i just starded the anime that shit is so good  ||   mr x l2u yeah bro  something in that anime hits different ||   massivegat5087agreed  the prelude is a masterpiece  one of the best if not the best when it comes to a revenge arc  pair that with farmland saga and you have some of the best character development that i have seen  || greatness || vegabond is underrated more people should read this manga || it was an awesome manga and it has a similiarity on the manga  berserk due to it s unique type of art style and also has a good plot of a story  many people should read this masterpiece || vinland saga too ||   darkzslash i could find only last chapter online which shows musashi as old and some young guy trying to assassinate him  still does n t show what happens between him and sasaki kojiro  although we all know musashi won in the end but that pending fight has been hyped for so long it s such a waste not to draw it after hinting at it for so long  i think mangaka is n t that old and is busy with real manga at the moment maybe he could draw 30 40 chapters and give us conclusion and happy ending after he finishes real  basically more than 90  manga is complete just needs to write last arc ',\n",
       " 'likes': 2892}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments = []\n",
    "for i in range(5):\n",
    "    dic = {}\n",
    "    dic['comment'] = top_comments_df.loc[i, \"cleaned_parent_comm\"]\n",
    "    dic['replies'] = top_comments_df.loc[i, \"cleaned_replies\"]\n",
    "    dic['likes'] = top_comments_df.loc[i, \"Likes\"]\n",
    "    top_comments.append(dic)\n",
    "top_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2f88f5-1721-4c64-8957-67e1b0491ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c73e25-382b-4cc7-a217-12bb18d4186b",
   "metadata": {},
   "source": [
    "### WE HAVE DATA NOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69ef96-c131-464f-9d90-fb6837592b14",
   "metadata": {},
   "source": [
    "### TOPIC MODELLING DOESNT CARE ABOUT THE START AND END OF SENTENCE SINCE IT USES BAG OF WORDS FOR MODELLING T`OPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad12e4a-9a26-4b4b-91c7-0a9a5cbb401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if np.nan: print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c156ca60-69fe-4294-828f-5cc48c89b231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thank you for my very existence'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e7506b9-ea53-48d8-a3df-d632e989aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = []\n",
    "for sent in all_text:\n",
    "    if type(sent) == float: continue\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(sent)  # Convert to lowercase for consistency\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into a sentence\n",
    "    text.append(\" \".join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "637d4f6b-c799-4971-86cd-cdc194343ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank existence'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a71fa03f-fb15-4aa2-9bb9-b348f68ee446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting top2vec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached top2vec-1.0.36-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from top2vec) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from top2vec) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from top2vec) (1.6.1)\n",
      "Collecting gensim>=4.0.0 (from top2vec)\n",
      "  Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting umap-learn>=0.5.1 (from top2vec)\n",
      "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting hdbscan>=0.8.27 (from top2vec)\n",
      "  Using cached hdbscan-0.8.40-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wordcloud (from top2vec)\n",
      "  Using cached wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from top2vec) (4.48.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from top2vec) (4.67.1)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim>=4.0.0->top2vec)\n",
      "  Using cached scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from gensim>=4.0.0->top2vec) (7.1.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from hdbscan>=0.8.27->top2vec) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from scikit-learn>=1.2.0->top2vec) (3.5.0)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.1->top2vec)\n",
      "  Using cached numba-0.61.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->top2vec)\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from pandas->top2vec) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from pandas->top2vec) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from pandas->top2vec) (2024.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from tqdm->top2vec) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from transformers->top2vec) (0.5.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from wordcloud->top2vec) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from wordcloud->top2vec) (3.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers->top2vec) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers->top2vec) (4.12.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn>=0.5.1->top2vec)\n",
      "  Using cached llvmlite-0.44.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->top2vec) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from smart-open>=1.8.1->gensim>=4.0.0->top2vec) (1.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from requests->transformers->top2vec) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from requests->transformers->top2vec) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from requests->transformers->top2vec) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varshil\\miniconda3\\envs\\ganlab\\lib\\site-packages (from requests->transformers->top2vec) (2024.12.14)\n",
      "Using cached top2vec-1.0.36-py3-none-any.whl (33 kB)\n",
      "Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "Using cached hdbscan-0.8.40-cp312-cp312-win_amd64.whl (726 kB)\n",
      "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Using cached wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Using cached numba-0.61.0-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "Using cached llvmlite-0.44.0-cp312-cp312-win_amd64.whl (30.3 MB)\n",
      "Installing collected packages: scipy, llvmlite, numba, gensim, wordcloud, pynndescent, hdbscan, umap-learn, top2vec\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n"
     ]
    }
   ],
   "source": [
    "!pip install top2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93558c-4238-4e49-bcef-b4827e3b3fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"from bertopic import BERTopic\n",
    "\n",
    "# Example: Unstructured text (Reddit comments, YouTube comments, news, etc.)\n",
    "docs = [\n",
    "    \"Bitcoin is going to the moon! Crypto is booming.\",\n",
    "    \"Ethereum is the future of blockchain technology.\",\n",
    "    \"I love playing football on weekends.\",\n",
    "    \"Messi and Ronaldo are the greatest footballers ever.\",\n",
    "    \"Stock market is very volatile these days.\",\n",
    "    \"Investing in cryptocurrency can be risky but rewarding.\"\n",
    "]\n",
    "\n",
    "# Initialize and train BERTopic\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Show most discussed topics\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "==> OSError: [WinError 182] The operating system cannot run %1. Error loading \"C:\\Users\\Varshil\\miniconda3\\envs\\NLP-Tx\\lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eeb52c7-c9ee-4c4a-9382-88a910e8658f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Generate Embeddings\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_openai_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 3. Reduce Dimensionality\u001b[39;00m\n\u001b[0;32m     37\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# Reduce from 1536D to 50D for better clustering\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m, in \u001b[0;36mget_openai_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_openai_embeddings\u001b[39m(texts):\n\u001b[1;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m(\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtexts, \n\u001b[0;32m     28\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m     )\n\u001b[0;32m     30\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\NLP-Tx\\lib\\site-packages\\openai\\_utils\\_proxy.py:20\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\NLP-Tx\\lib\\site-packages\\openai\\_utils\\_proxy.py:55\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\NLP-Tx\\lib\\site-packages\\openai\\_module_client.py:54\u001b[0m, in \u001b[0;36mEmbeddingsProxy.__load__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources\u001b[38;5;241m.\u001b[39mEmbeddings:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\NLP-Tx\\lib\\site-packages\\openai\\__init__.py:329\u001b[0m, in \u001b[0;36m_load_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m    313\u001b[0m         _client \u001b[38;5;241m=\u001b[39m _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    314\u001b[0m             api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[0;32m    315\u001b[0m             azure_endpoint\u001b[38;5;241m=\u001b[39mazure_endpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m             http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[0;32m    326\u001b[0m         )\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[1;32m--> 329\u001b[0m     _client \u001b[38;5;241m=\u001b[39m \u001b[43m_ModuleClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\NLP-Tx\\lib\\site-packages\\openai\\_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73cd8028-3a83-4925-bf16-e39c178971f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8580ddd62a7d47c9b3b838a65eeabea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varshil\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Varshil\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e76d02e89742a59bbba0e355224c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66452480d274482bba7790689aecba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463ef62d7bb14d0d8f0093c5b8a7bf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470168f4eb9f4ad1b213ad6c9556d045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=50 must be between 0 and min(n_samples, n_features)=10 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 3. Reduce Dimensionality\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# Reduce from 768D to 50D for better clustering\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m reduced_embeddings \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(embeddings)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 4. Apply Clustering (HDBSCAN for adaptive topic discovery)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m clusterer \u001b[38;5;241m=\u001b[39m hdbscan\u001b[38;5;241m.\u001b[39mHDBSCAN(min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components, xp, is_array_api_compliant)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:556\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=50 must be between 0 and min(n_samples, n_features)=10 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Hugging Face Transformer model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc0e5df0-2480-41a3-bca7-1a005ef8cd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "852d718d-b9c1-4bf1-af23-f0d01f1030c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3070230528 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Use [CLS] token embedding\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n\u001b[1;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_transformer_embeddings(text)\n",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m, in \u001b[0;36mget_transformer_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Use [CLS] token embedding\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1143\u001b[0m     embedding_output,\n\u001b[0;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1145\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1146\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1147\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1148\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1149\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1150\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1151\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1153\u001b[0m )\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    696\u001b[0m         hidden_states,\n\u001b[0;32m    697\u001b[0m         attention_mask,\n\u001b[0;32m    698\u001b[0m         layer_head_mask,\n\u001b[0;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    701\u001b[0m         past_key_value,\n\u001b[0;32m    702\u001b[0m         output_attentions,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    586\u001b[0m         hidden_states,\n\u001b[0;32m    587\u001b[0m         attention_mask,\n\u001b[0;32m    588\u001b[0m         head_mask,\n\u001b[0;32m    589\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    590\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[0;32m    518\u001b[0m         head_mask,\n\u001b[0;32m    519\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    521\u001b[0m         past_key_value,\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\GANLAB\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    438\u001b[0m )\n\u001b[1;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    441\u001b[0m     query_layer,\n\u001b[0;32m    442\u001b[0m     key_layer,\n\u001b[0;32m    443\u001b[0m     value_layer,\n\u001b[0;32m    444\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    445\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    446\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    447\u001b[0m )\n\u001b[0;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3070230528 bytes."
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Generate Embeddings with Hugging Face Transformers\n",
    "def get_transformer_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Use [CLS] token embedding\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_transformer_embeddings(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0518c28-5b68-4a94-8b61-6e865d12efb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b675365-fa12-4b63-9e57-e3fbad837cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reduce Dimensionality\n",
    "pca = PCA(n_components=50)  # Reduce from 768D to 50D for better clustering\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# 4. Apply Clustering (HDBSCAN for adaptive topic discovery)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')\n",
    "labels = clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "# 5. Assign Topics to Clusters (Nearest Neighbors for Top Words)\n",
    "def extract_top_keywords(embeddings, docs, labels, n_neighbors=3):\n",
    "    unique_labels = set(labels)\n",
    "    topic_keywords = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label == -1:  # Ignore noise points\n",
    "            continue\n",
    "        \n",
    "        cluster_docs = [docs[i] for i in range(len(docs)) if labels[i] == label]\n",
    "        cluster_embeddings = [embeddings[i] for i in range(len(docs)) if labels[i] == label]\n",
    "        \n",
    "        knn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine')\n",
    "        knn.fit(cluster_embeddings)\n",
    "        distances, indices = knn.kneighbors(cluster_embeddings)\n",
    "        \n",
    "        topic_keywords[label] = [cluster_docs[i] for i in indices[0]]\n",
    "    \n",
    "    return topic_keywords\n",
    "\n",
    "# Extract keywords\n",
    "topics = extract_top_keywords(reduced_embeddings, documents, labels)\n",
    "\n",
    "# 6. Visualize Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], hue=labels, palette='tab10')\n",
    "plt.title(\"Topic Clusters\")\n",
    "plt.show()\n",
    "\n",
    "# Print Topics\n",
    "for topic, keywords in topics.items():\n",
    "    print(f\"Topic {topic}: {keywords}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
